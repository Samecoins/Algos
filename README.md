
---

# Методы анализа алгоритмов

## Временная сложность и O-нотация

* **Определение временной сложности.** В информатике временная сложность алгоритма определяется как функция от длины входных данных, равная времени работы алгоритма на данном входе. Обычно сложность выражается через O-нотацию, в которой учитывается только слагаемое самого высокого порядка и игнорируются константные множители. Например, если для входов длины $n$ время работы не превосходит $5n^3+3n$, то временная сложность асимптотически оценивается как $O(n^3)$. При оценке подсчитывают количество *элементарных операций*; время исполнения одной операции принимают за константу, поэтому полное время и число операций отличаются максимум на постоянный множитель. О-нотация чаще всего применяется к **худшему** случаю, обозначаемому $T(n)$, но иногда анализируют средний случай.
* **Нотации $\Theta$ и $\Omega$.** Помимо O-нотации используют $\Theta$-нотацию (двусторонняя оценка, когда время $T(n)$ ограничено сверху и снизу одной и той же функцией) и $\Omega$-нотацию (нижняя оценка). Эти нотации позволяют сравнивать алгоритмы по скорости роста их временных затрат.
* **Мастер-теорема.** Для алгоритмов типа «разделяй и властвуй» возникают рекуррентные соотношения вида
  $T(n)=aT(n/b)+f(n),$
  где $a$ — число подзадач, $n/b$ — размер каждой подзадачи, $f(n)$ — работа вне рекурсивных вызовов. *Основная теорема о рекуррентных соотношениях* (master-теорема) используется для получения асимптотических оценок таких рекурсий. Например, если $f(n)=O(n^c)$ и $c<\log_b a$, то $T(n)=\Theta(n^{\log_b a})$. Этот подход позволяет анализировать сложность многих алгоритмов без полного раскрытия рекурсии.

## Быстрое умножение длинных чисел

### Алгоритм Карацубы

* **Идея.** Тривиальное умножение двух $n$-разрядных чисел требует $O(n^2)$ однобитовых умножений. Алгоритм Карацубы использует принцип «разделяй и властвуй»: число представляют как $ax+b$ и $cx+d$, где $x=2^{\lceil n/2\rceil}$, и сводят вычисление $(ax+b)(cx+d)$ к трём (а не четырём) произведениям более коротких чисел. В результате рекуррентная оценка времени работы имеет вид
  $T(n)=3T(n/2)+O(n),$
  откуда следует $T(n)=O(n^{\log_2 3})\approx O(n^{1.585}).$
  Эта оценка лучше квадратичной; поэтому для длинных чисел метод Карацубы эффективнее наивного умножения.
* **Сравнение с классическим методом.** Классический метод выполняет четыре умножения $ac$, $ad$, $bc$ и $bd$; его рекуррентное соотношение
  $T(n)=4T(n/2)+O(n)$
  даёт сложность $O(n^2)$. Отказ от одного умножения в пользу дополнительных сложений/вычитаний улучшает асимптотику.

### Алгоритм Шёнхаге–Штрассена

Алгоритм Шёнхаге–Штрассена основан на быстром преобразовании Фурье и умножает два $n$-разрядных числа за $O(n\log n\log\log n)$ битовых операций. Сложение $n$-разрядных двоичных чисел требует $O(n)$ битовых операций, а алгоритм Шёнхаге–Штрассена выполняет умножение за $O(n\log n\log\log n)$ битовых операций, что значительно быстрее квадратического метода. Хотя этот алгоритм сложен для реализации, он используется в библиотеке GMP и в символьных пакетах для очень длинных целых.

## Алгоритм Штрассена для умножения матриц

Алгоритм Штрассена — первый алгоритм, который умножает квадратные матрицы быстрее $O(n^3)$. Он разбивает матрицы $A$ и $B$ на блоки и вычисляет семь произведений блоков вместо восьми, как в классическом алгоритме. Классическое умножение тратит время $\Theta(n^{\log_2 8})=\Theta(n^3)$, тогда как алгоритм Штрассена умножает матрицы за $\Theta(n^{\log_2 7}) \approx O(n^{2.81})$. Рекурсивное соотношение для его времени работы
$T(n)=7T(n/2)+O(n^2)$
приводит к $T(n)=O(n^{\log_2 7}).$
Несмотря на то, что существуют ещё более быстрые методы (Копперсмита–Винограда и др.), алгоритм Штрассена прост в реализации и часто используется для достаточно больших плотных матриц.

## Принципы анализа на примерах

* **Постановка рекуррентных соотношений.** Для рекурсивных алгоритмов, таких как Карацуба и Штрассен, первый шаг анализа — записать время работы $T(n)$ через времена подзадач и затраты вне рекурсивных вызовов. Например, для Карацубы $T(n)=3T(n/2)+O(n)$, а для Штрассена $T(n)=7T(n/2)+O(n^2)$. Затем применяют мастер-теорему.
* **Худший и лучший случаи.** В сортировке пузырьком и вставками лучшие случаи (почти отсортированный массив) требуют $O(n)$ сравнений, тогда как худшие случаи (массив в обратном порядке) требуют $O(n^2)$ сравнений и пересылок. Анализ учитывает типичные случаи и оптимизации (например, бинарные вставки уменьшают число сравнений до $O(n\log n)$, но количество перемещений остаётся $O(n^2)$).
* **Нижние оценки.** Для многих задач интересны не только верхние оценки, но и доказательства того, что алгоритм не может быть существенно быстрее. Примером является теорема о нижней границе для сортировки сравнением: любой алгоритм, основанный только на сравнениях, в худшем случае выполняет $\Omega(n\log n)$ сравнений. Доказательство использует *дерево решений* — любой алгоритм сравнения можно представить как бинарное дерево, высота которого определяет количество сравнений; поскольку существует $n!$ различных перестановок, высота дерева должна быть не меньше $\log_2(n!)\approx \Omega(n\log n)$.

---

# Алгоритмы внутренней сортировки

## Что такое внутренняя сортировка

Внутренняя сортировка (internal sort) — реализация алгоритма сортировки, при которой объёма оперативной памяти достаточно, чтобы разместить весь сортируемый массив и выполнять алгоритм с произвольным доступом к элементам. В этом случае сортировка проходит быстрее, чем внешняя сортировка, поскольку доступ к оперативной памяти значительно быстрее, чем к периферийным устройствам.

Внутренние сортировки могут работать *in-place* (не используют дополнительную память) или требовать $O(n)$ дополнительной памяти (например, сортировка слиянием). К базовым внутренним алгоритмам относят сортировки обменами (пузырьком, перемешиванием), вставками (вставками, Шелла), выбором (выбором, пирамидальная/heap sort), а также сортировки «разделяй и властвуй» (quick sort, merge sort) и гибридные методы.

## Простейшие алгоритмы внутренней сортировки

### Сортировка пузырьком (bubble sort)

Алгоритм многократно проходит по массиву; на каждой итерации он последовательно сравнивает соседние элементы и меняет их местами, если они расположены в неверном порядке. Минимум один элемент «всплывает» на своё место за проход, поэтому в худшем случае требуется $n{-}1$ проходов. Неоптимизированный вариант выполняет $(n{-}1)^2$ сравнений, то есть $O(n^2)$. Количество обменов равно числу инверсий в исходном массиве; в худшем случае это $\frac{n(n-1)}{2}$, поэтому время обменов также $O(n^2)$. Оптимизации (сокращение внутреннего цикла и прекращение работы при отсутствии обменов) улучшают поведение на почти отсортированных массивах, но асимптотика остаётся $O(n^2)$.

### Сортировка вставками (insertion sort)

Вставки поддерживают часть массива отсортированной и по одному помещают элементы из неотсортированной области на нужные позиции. Элемент $a[i]$ сравнивают с предшествующими элементами $a[i-1],a[i-2],\dots$, пока найденный элемент не окажется меньше или пока не будет достигнут начало массива; затем $a[i]$ вставляют в эту позицию. Этот метод также работает за $O(n + k)$, где $k$ — число обменов; в среднем и худшем случае $k=\Theta(n^2)$. Если массив почти отсортирован, затраты падают до $O(n)$. Бинарные (двухпутевые) вставки уменьшают число сравнений до $O(n\log n)$, но перемещения по-прежнему требуют $O(n^2)$ времени.

### Сортировка выбором (selection sort)

На $i$-м шаге алгоритм находит минимальный (или максимальный) элемент среди ещё неотсортированных и меняет его местами с элементом на позиции $i$. В простейшем варианте каждый шаг выполняет $O(n-i)$ обменов (поскольку каждый найденный элемент сразу меняется с $a[i]$), что даёт $O(n^2)$ пересылок. Улучшенный вариант находит минимум за один проход и делает ровно один обмен на шаг, т.е. выполняет $O(n)$ обменов, но количество сравнений остаётся $O(n^2)$. Алгоритм работает in-place, но неустойчив и редко применяется на практике из-за квадратичной сложности.

### Сортировка Шелла

Этот алгоритм является развитием метода вставок, в котором элементы сначала сортируют на больших расстояниях (заданная последовательность шагов $h_t$), а затем шаг постепенно уменьшается до 1. Применяя на каждом шаге сортировку вставками, метод Шелла уменьшает число перемещений и сравнений. При удачном выборе последовательности шагов сложность может быть существенно лучше $O(n^2)$, хотя в простейших реализациях она может оставаться квадратичной.

### Пирамидальная сортировка (heap sort)

Heap sort строит двоичную кучу (heap) из массива за $O(n)$, затем $n{-}1$ раз извлекает максимум (корень кучи) и помещает его в конец массива. Каждый вызов `siftDown` восстанавливает свойства кучи за $O(\log n)$, поэтому общая сложность $O(n\log n)$. Алгоритм требует только $O(1)$ дополнительной памяти, но является неустойчивым, а на почти отсортированных массивах работает так же долго, как и на случайных данных.

### Быстрая сортировка (quick sort)

Быстрая сортировка работает по принципу «разделяй и властвуй»: она выбирает опорный элемент, разбивает массив на элементы меньшие и большие опорного, и рекурсивно сортирует каждую часть. Среднее время работы — $O(n\log n)$, но в худшем случае (несбалансированное разбиение) — $\Theta(n^2)$. Массив в обратном порядке или неудачный выбор опорного элемента приводит к худшему случаю; тогда рекуррентность $T(n)=T(n{-}1)+\Theta(n)$ раскрывается в квадратичную сложность. На практике quick sort — один из самых быстрых методов сортировки; его модификации (randomized quick sort, introsort) устраняют худшие случаи.

### Сортировка слиянием (merge sort)

Merge sort делит массив пополам, рекурсивно сортирует половины и сливает их. Время работы удовлетворяет рекуррентному соотношению
$
T(n)=2T(n/2)+O(n),
$
решение даёт $T(n)=O(n\log n)$. Merge sort стабильна и подходит для внешней сортировки, но требует $O(n)$ дополнительной памяти. Итеративная версия уменьшает объём памяти, которая раньше тратилась на рекурсивные вызовы, но всё равно использует вспомогательный массив.

## Сравнительная таблица

| Алгоритм                       | Лучшее время                             | Среднее время | Худшее время | Память             | Устойчивость | Краткое описание                                                                                              |
| ------------------------------ | ---------------------------------------- | ------------- | ------------ | ------------------ | ------------ | ------------------------------------------------------------------------------------------------------------- |
| **Пузырьком (bubble sort)**    | $O(n)$ (при уже отсортированном массиве) | $O(n^2)$      | $O(n^2)$     | $O(1)$             | Да           | Проходы по массиву; соседи меняются местами, если стоят в неверном порядке.                                   |
| **Вставками (insertion sort)** | $O(n)$                                   | $O(n^2)$      | $O(n^2)$     | $O(1)$             | Да           | Поддерживает отсортированную часть; каждый элемент вставляется на нужную позицию в отсортированную подмассив. |
| **Выбором (selection sort)**   | $O(n^2)$                                 | $O(n^2)$      | $O(n^2)$     | $O(1)$             | Нет          | На каждом шаге ищет минимальный элемент среди неотсортированных и ставит его в правильную позицию.            |
| **Шелла**                      | Зависит от последовательности шагов      | —             | До $O(n^2)$  | $O(1)$             | Нет          | Сортировка вставками для элементов, находящихся на определённых расстояниях; шаг уменьшается до 1.            |
| **Heapsort**                   | $O(n\log n)$                             | $O(n\log n)$  | $O(n\log n)$ | $O(1)$             | Нет          | Строит кучу, затем многократно извлекает максимум; использует `siftDown` за $O(\log n)$.                      |
| **Quicksort**                  | $O(n\log n)$                             | $O(n\log n)$  | $O(n^2)$     | $O(\log n)$ (стек) | Нет          | Разбивает массив относительно опорного элемента и рекурсивно сортирует части.                                 |
| **Merge sort**                 | $O(n\log n)$                             | $O(n\log n)$  | $O(n\log n)$ | $O(n)$             | Да           | Делит массив пополам, сортирует части и сливает; рекуррентность $T(n)=2T(n/2)+O(n)$ даёт $O(n\log n)$.        |

## Теорема о нижней границе для сортировки сравнением

Теорема утверждает, что любой алгоритм сортировки, основанный исключительно на сравнениях, в худшем случае выполняет не менее $\Omega(n\log n)$ сравнений. Доказательство: алгоритм сравнения можно представить как бинарное дерево, где узлы соответствуют операциям сравнения, а листья — перестановкам элементов. Поскольку существует $n!$ различных перестановок, дерево должно иметь не менее $n!$ листьев, а двоичное дерево высоты $h$ имеет не более $2^h$ листьев. Из неравенства $n!\le 2^h$ следует $h\ge \log_2 n!\ge \Omega(n\log n)$. Следовательно, любой корректный алгоритм сортировки сравнениями имеет время работы $\Omega(n\log n)$, а такие алгоритмы, как heapsort и merge sort, достигают этой нижней границы, что делает их асимптотически оптимальными.
